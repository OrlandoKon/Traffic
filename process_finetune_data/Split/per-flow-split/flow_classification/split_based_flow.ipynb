{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset\n",
    "\n",
    "This is for Split data to three datasets (train, val, test)\n",
    "\n",
    "All flows are disjointly, the test distribution is same with origin datasets\n",
    "\n",
    "Train and Test datasets are balanced, which means every class has similar number packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scapy.all as scapy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import json\n",
    "\n",
    "os.chdir('/root/data')\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(       \n",
    "    level=logging.INFO,            \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  \n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/split_based_flow.log', mode='w'),  \n",
    "        logging.StreamHandler()          \n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 1 # k-folder\n",
    "seed = 43 # random seed\n",
    "threshold = 5 # threshold for the number of packets in a flow\n",
    "test_size = 1/(k+1) # test size for train-test split\n",
    "random.seed(seed)\n",
    "\n",
    "dataset = 'ISCX-VPN-2016'\n",
    "dataset_path = 'ISCX-VPN-2016/filtered/raw'\n",
    "output_path = 'ISCX-VPN-2016/filtered/flow'\n",
    "\n",
    "if dataset == 'ISCX-VPN-2016':\n",
    "    datasets_class_name = ['aim', 'email', 'facebook', 'sftp', 'gmail', 'hangout', 'icq', 'netflix', 'scp', 'ftp', 'skype', 'spotify', 'vimeo', 'torrent', 'voipbuster', 'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:24:18,226 - root - INFO - Processing folder: netflix\n",
      "2025-12-09 12:24:18,226 - root - INFO - Class name: netflix\n",
      "2025-12-09 12:24:18,284 - root - INFO - Processing folder: spotify\n",
      "2025-12-09 12:24:18,285 - root - INFO - Class name: spotify\n",
      "2025-12-09 12:24:18,341 - root - INFO - Processing folder: skype\n",
      "2025-12-09 12:24:18,342 - root - INFO - Class name: skype\n",
      "2025-12-09 12:24:18,401 - root - INFO - Processing folder: ftp\n",
      "2025-12-09 12:24:18,402 - root - INFO - Class name: ftp\n",
      "2025-12-09 12:24:18,455 - root - INFO - Processing folder: voipbuster\n",
      "2025-12-09 12:24:18,456 - root - INFO - Class name: voipbuster\n",
      "2025-12-09 12:24:18,508 - root - INFO - Processing folder: sftp\n",
      "2025-12-09 12:24:18,509 - root - INFO - Class name: sftp\n",
      "2025-12-09 12:24:18,564 - root - INFO - Processing folder: vimeo\n",
      "2025-12-09 12:24:18,565 - root - INFO - Class name: vimeo\n",
      "2025-12-09 12:24:18,615 - root - INFO - Processing folder: torrent\n",
      "2025-12-09 12:24:18,616 - root - INFO - Class name: torrent\n",
      "2025-12-09 12:24:18,667 - root - INFO - Processing folder: youtube\n",
      "2025-12-09 12:24:18,668 - root - INFO - Class name: youtube\n",
      "2025-12-09 12:24:18,724 - root - INFO - Processing folder: hangout\n",
      "2025-12-09 12:24:18,725 - root - INFO - Class name: hangout\n",
      "2025-12-09 12:24:18,785 - root - INFO - Processing folder: icq\n",
      "2025-12-09 12:24:18,786 - root - INFO - Class name: icq\n",
      "2025-12-09 12:24:18,843 - root - INFO - Processing folder: scp\n",
      "2025-12-09 12:24:18,844 - root - INFO - Class name: scp\n",
      "2025-12-09 12:24:18,899 - root - INFO - Processing folder: aim\n",
      "2025-12-09 12:24:18,900 - root - INFO - Class name: aim\n",
      "2025-12-09 12:24:18,952 - root - INFO - Processing folder: email\n",
      "2025-12-09 12:24:18,953 - root - INFO - Class name: email\n",
      "2025-12-09 12:24:19,005 - root - INFO - Processing folder: facebook\n",
      "2025-12-09 12:24:19,006 - root - INFO - Class name: facebook\n",
      "2025-12-09 12:24:19,064 - root - INFO - Processing folder: gmail\n",
      "2025-12-09 12:24:19,065 - root - INFO - Class name: gmail\n",
      "2025-12-09 12:24:19,118 - root - INFO - Results saved to results.json\n",
      "2025-12-09 12:24:19,119 - root - INFO - Data processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# statistics the information of the dataset\n",
    "# num_of_class = len(datasets_class_name)\n",
    "num_of_original_flow = 0\n",
    "num_of_filtered_flow = 0\n",
    "num_of_flow_per_class = defaultdict(int) # number of flows per class, which is larger than threshold\n",
    "flow_file_of_class = defaultdict(list) # list of flow files per class\n",
    "\n",
    "def process_flow(args):\n",
    "    flow, folder_path, class_name = args\n",
    "    count = 0\n",
    "    with scapy.PcapReader(f\"{folder_path}/{flow}\") as packets:\n",
    "        for _ in packets:\n",
    "            count += 1\n",
    "            if count >= threshold:\n",
    "                return class_name, f\"{folder_path}/{flow}\"\n",
    "        \n",
    "for folder in os.listdir(dataset_path): # AIM_chat_1 directory\n",
    "    logger.info(f\"Processing folder: {folder}\")\n",
    "    if dataset == 'tls' or dataset == 'vpn-app':\n",
    "        class_name = folder\n",
    "    else:\n",
    "        class_name = next((name for name in datasets_class_name if name in folder or name.upper() in folder), None)\n",
    "        if class_name is None:\n",
    "            logger.warning(f\"No matching class for folder: {folder}\")\n",
    "            continue\n",
    "    logger.info(f\"Class name: {class_name}\")\n",
    "\n",
    "    flows = [flow for flow in os.listdir(f\"{dataset_path}/{folder}\")] # all flow\n",
    "    num_of_original_flow += len(flows)\n",
    "\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_flow, [(flow, f\"{dataset_path}/{folder}\", class_name) for flow in flows])\n",
    "\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            class_name, flow = result\n",
    "            num_of_filtered_flow += 1\n",
    "            num_of_flow_per_class[class_name] += 1\n",
    "            flow_file_of_class[class_name].append(flow)\n",
    "\n",
    "results = {\n",
    "    'num_of_original_flow': num_of_original_flow,\n",
    "    'num_of_filtered_flow': num_of_filtered_flow,\n",
    "    'num_of_flow_per_class': dict(num_of_flow_per_class),\n",
    "    'flow_file_of_class': dict(flow_file_of_class)\n",
    "}\n",
    "\n",
    "with open(f'statistics/{dataset}_{threshold}_flow.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "logger.info('Results saved to results.json')\n",
    "logger.info('Data processing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'netflix': 1, 'spotify': 1, 'skype': 6, 'ftp': 1, 'voipbuster': 1, 'sftp': 2, 'vimeo': 1, 'torrent': 0, 'youtube': 2, 'hangout': 3, 'icq': 1, 'scp': 3, 'aim': 1, 'email': 1, 'facebook': 4, 'gmail': 0}\n",
      "{'netflix': 1, 'spotify': 1, 'skype': 1, 'ftp': 1, 'voipbuster': 1, 'sftp': 1, 'vimeo': 1, 'torrent': 1, 'youtube': 1, 'hangout': 1, 'icq': 1, 'scp': 1, 'aim': 1, 'email': 1, 'facebook': 1, 'gmail': 1}\n"
     ]
    }
   ],
   "source": [
    "test_num_of_flow_per_class = {key: int(test_size * value) for key, value in num_of_flow_per_class.items()}\n",
    "\n",
    "train_val_num_of_flow_per_class = {key: value - test_num_of_flow_per_class[key] for key, value in num_of_flow_per_class.items()}\n",
    "\n",
    "if dataset == 'tls': # because the latest class is too small, we use the second smallest class\n",
    "    sorted_values = sorted(train_val_num_of_flow_per_class.values())\n",
    "    min_train_num_of_flow_per_class= sorted_values[1]\n",
    "else:\n",
    "    min_train_num_of_flow_per_class = min(train_val_num_of_flow_per_class.values())   \n",
    "\n",
    "train_val_num_of_flow_per_class = {key: min(min_train_num_of_flow_per_class, value) for key, value in train_val_num_of_flow_per_class.items()}\n",
    "\n",
    "print(test_num_of_flow_per_class)\n",
    "print(train_val_num_of_flow_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the test and train-val set\n",
    "results = {}\n",
    "train_val_flow_file_of_class = defaultdict(list)\n",
    "test_flow_file_of_class = defaultdict(list)\n",
    "\n",
    "for class_name, flow_files in flow_file_of_class.items():\n",
    "    test_flow_file_of_class[class_name] = random.sample(flow_files, test_num_of_flow_per_class[class_name])\n",
    "\n",
    "for class_name, flow_files in flow_file_of_class.items():\n",
    "    train_val_flow_file_of_class[class_name] = list(set(flow_files) - set(test_flow_file_of_class[class_name]))\n",
    "\n",
    "k_folds = defaultdict(list)\n",
    "for class_name, flow_files in train_val_flow_file_of_class.items():\n",
    "    random.shuffle(flow_files)\n",
    "    fold_size = train_val_num_of_flow_per_class[class_name] // k\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        k_folds[class_name].append(flow_files[start:end])\n",
    "\n",
    "for i in range(k):\n",
    "    results[f'k_{i}'] = {class_name: k_folds[class_name][i] for class_name in k_folds}\n",
    "results['test'] = test_flow_file_of_class\n",
    "\n",
    "with open(f'outputs/{dataset}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:58:56,986 - root - INFO - Copying test files of class netflix\n",
      "2025-12-08 16:58:58,736 - root - INFO - Copying test files of class spotify\n",
      "2025-12-08 16:58:58,758 - root - INFO - Copying test files of class skype\n",
      "2025-12-08 16:58:59,584 - root - INFO - Copying test files of class ftp\n",
      "2025-12-08 17:01:08,621 - root - INFO - Copying test files of class voipbuster\n",
      "2025-12-08 17:01:09,089 - root - INFO - Copying test files of class sftp\n",
      "2025-12-08 17:01:22,559 - root - INFO - Copying test files of class vimeo\n",
      "2025-12-08 17:01:23,212 - root - INFO - Copying test files of class torrent\n",
      "2025-12-08 17:01:23,213 - root - INFO - Copying test files of class youtube\n",
      "2025-12-08 17:01:23,873 - root - INFO - Copying test files of class hangout\n",
      "2025-12-08 17:01:48,284 - root - INFO - Copying test files of class icq\n",
      "2025-12-08 17:01:48,384 - root - INFO - Copying test files of class scp\n",
      "2025-12-08 17:02:47,143 - root - INFO - Copying test files of class aim\n",
      "2025-12-08 17:02:47,287 - root - INFO - Copying test files of class email\n",
      "2025-12-08 17:02:47,310 - root - INFO - Copying test files of class facebook\n",
      "2025-12-08 17:02:50,650 - root - INFO - Copying test files of class gmail\n"
     ]
    }
   ],
   "source": [
    "# create the output directory of the test dataset\n",
    "os.makedirs(f\"{output_path}/test\", exist_ok=True)\n",
    "\n",
    "for class_name, flow_files in test_flow_file_of_class.items():\n",
    "    logger.info(f\"Copying test files of class {class_name}\")\n",
    "    os.makedirs(f\"{output_path}/test/{class_name}\", exist_ok=True)\n",
    "    for flow_file in flow_files:\n",
    "        command_result = os.system(f\"cp {flow_file} {output_path}/test/{class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the output directory of the train-val dataset\n",
    "for i in range(k):\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}/val\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_path}/train_val_split_{i}/train\", exist_ok=True)\n",
    "\n",
    "    for class_name, flow_files in k_folds.items():\n",
    "        logger.info(f\"Copying val files {i} of class {class_name} in split {i}\")\n",
    "        os.makedirs(f\"{output_path}/train_val_split_{i}/val/{class_name}\", exist_ok=True)\n",
    "\n",
    "        for flow_file in flow_files[i]:\n",
    "            command_result = os.system(f\"cp {flow_file} {output_path}/train_val_split_{i}/val/{class_name}\")\n",
    "\n",
    "\n",
    "    other_index = list(range(i)) + list(range(i+1, k))\n",
    "    for j in other_index:\n",
    "        for class_name, flow_files in k_folds.items():\n",
    "            logger.info(f\"Copying train files {j} of class {class_name} in split {i}\")\n",
    "            os.makedirs(f\"{output_path}/train_val_split_{i}/train/{class_name}\", exist_ok=True)\n",
    "\n",
    "            for flow_file in flow_files[j]:\n",
    "                command_result = os.system(f\"cp {flow_file} {output_path}/train_val_split_{i}/train/{class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etbert",
   "language": "python",
   "name": "etbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
